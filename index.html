<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Diff-ID</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://ku-cvlab.github.io/Diff-ID/">
    <meta property="og:title" content="Pose-Diversified Augmentation with Diffusion Model for Person Re-Identification">
    <meta property="og:description" content="">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                Pose-Diversified Augmentation with Diffusion Model <br> for Person Re-Identification <br>
                <small>
                    Arxiv 2024
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <a style="text-decoration:none" href="https://ines-hyeonsu-kim.github.io/">
                    Ine&#768;s Hyeonsu&nbsp;Kim<sup>1,</sup>*
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="">
                    JoungBin&nbsp;Lee<sup>1,</sup>*
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://github.com/soow0n">
                    Soowon&nbsp;Son<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://github.com/wooj0216">
                    Woojeong&nbsp;Jin<sup>1</sup>
                </a><br>
                <a style="text-decoration:none" href="">
                    Kyusun&nbsp;Cho<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://j0seo.github.io/">
                    Junyoung&nbsp;Seo<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="">
                    Min-Seop&nbsp;Kwak<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="">
                    Seokju&nbsp;Cho<sup>1</sup>
                </a><br>
                <a style="text-decoration:none" href="">
                    JeonYeol&nbsp;Baek<sup>2</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="">
                    Byeongwon&nbsp;Lee<sup>2</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://cvlab.korea.ac.kr">
                    Seungryong&nbsp;Kim<sup>1</sup>
                </a>
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <sup>1</sup>Korea University
                        </td>
                        <td>
                            <sup>2</sup>SKT
                        </td>
                    </tr>
                </table>
                <small>
                    *Equal Contribution
                </small>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/KU-CVLAB/Diff-ID" target="_blank">
                                <img src="img/github.png" height="60px">
                                    <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    Overview
                </h3> -->
                <div class="text-center">
                    <div class="text-justify">
                        <img src="./img/motivation.png" width="100%">
                    </div>
                    <br>
                    <div class="text-justify">
                        Upon observing the highly biased viewpoint and human
                        pose distributions in training dataset, we augment the dataset by manipulating SMPL body shapes
                        and feeding the rendered shapes into a generative model to fill in sparsely distributed poses and
                        viewpoints. With this augmented dataset, we can train a Re-ID model that is robust to viewpoint and
                        human pose biases.
                    </div>
                </div>
                
                <br>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Person re-identification (Re-ID) often faces challenges due to variations in human pose and camera viewpoint, which significantly affects the appearance variations of individuals across images. Existing datasets often lack diversity and scalability of these human pose and camera viewpoint, hindering the generalization of Re-ID models to new camera networks. Previous methods have attempted to address these issues by using data augmentation, but they rely on poses already present in the dataset, failing to effectively reduce the pose bias in the dataset. In this paper, we propose \textbf{Diff-ID}, a novel approach that augments training data with sparse and limited poses that are underrepresented in the original distribution. By leveraging the knowledge of pre-trained large-scale generative models like Stable Diffusion, we successfully generate realistic images with diverse human poses and camera viewpoints. Specifically, our objective is to create a training dataset that enables existing Re-ID models to learn features debiased to pose variations. Qualitative results demonstrate the effectiveness of our method in addressing pose bias and enhancing the generalizability of Re-ID models compared to other approaches. The performance gains achieved by training Re-ID models on our offline augmented dataset highlight the potential of our proposed framework in improving the scalability and generalizability of person Re-ID models.    
                </p>
            </div>
    
        </div>
        <br>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Main Architecture
                </h3>
                <div class="text-center">
                    <img src="./img/main_figure.png" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    Given the viewpoint and pose distributions, we first render the body shape sampled from the distribution using SMPL, 
                    generating the corresponding skeleton, depth map, and normal maps. These conditions, along with a reference image 
                    for identity preservation, are then fed into Diff-ID. Diff-ID consists of two branches: the reference U-Net processes 
                    the identity information from the reference image, while the denoising U-Net generates a person with the same identity, 
                    given the input conditions. The denoising U-Net generates images by iterating through the denoising process.
                </div>
                <br>
                <br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    The Effect of Viewpoint and Human Pose Augmentation 
                </h3>
                <div class="text-center">
                    <img src="./img/design_table.png" width="100%">
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/dist_vis.png" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    Visualization of camera viewpoint and human pose distributions for the Market-1501 and
                    DukeMTMC-reID datasets. The left figures (i) display the camera viewpoint distribution derived
                    from SMPL, while the right figures (ii) illustrate t-SNE visualizations of the human pose distributions.
                    These visualizations demonstrate that our pose augmentation successfully
                    diversifies both viewpoint and human pose distributions.
                </div>
                <br>
                <br>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Results
                </h3>
                <br>
                <div class="text-center">
                    <img src="./img/generated_images.png" width="80%">
                </div>
                <br>
                <div class="text-justify">
                    Given a reference image, we sample five images
                    generated by our method. These outputs demonstrate the model’s capability to produce diverse and
                    realistic variations. Ref. denotes the reference image.
                </div>
                <br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparison with GAN
                </h3>
                <div class="text-center">
                    <img src="./img/compare_quals.png" width="80%">
                </div>
                <br>
                <div class="text-justify">
                    Qualitative comparison for the generated output of FD-GAN and XingGAN. Our method demonstrates 
                    significantly better fidelity while faithfully capturing the identity of the person in the reference 
                    image and accurately following the target pose.
                </div>
                <br>
                <br>
                <div class="gan-compare-container">
                    <div class="left">
                        <img src="./img/compare_table.png" width="100%">
                    </div>
                    <div class="right">
                        <p>
                            Quantitative comparison on standard Re-ID benchmarks.
                            Note that the Re-ID Experts in the first row group are not directly comparable, as our primary focus is on dataset generation.
                            For augmentation-based methods, we train the same Re-ID model on the datasets generated by each
                            method to ensure a fair comparison.
                            <br>
                            *: The authors did not provide a pre-trained model.
                        </p>
                        
                    </div>
                </div>
                
                <br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                        citation
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                <!-- We would like to thank Lior Yariv and Kai Zhang for helping us evaluate their methods, and Ricardo Martin-Brualla for helpful comments on our text. DV is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>) -->
                    <!-- <br> -->
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
